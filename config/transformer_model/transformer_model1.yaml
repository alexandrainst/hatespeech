name: transformer_model1
model_dir: transformer_model
model_id: xlm-roberta-base
padding: longest
use_auth_token: False
cache_dir: .cache
from_flax: False
evaluation_strategy: steps
logging_strategy: steps
save_strategy: steps
eval_steps: 30
logging_steps: 30
save_steps: 30
max_steps: 10_000
report_to: none
save_total_limit: 1
batch_size: 32
learning_rate: 2e-5
warmup_ratio: 0.01
gradient_accumulation_steps: 1
load_best_model_at_end: True
optim: adamw_torch
metric: matthews_correlation
early_stopping_patience: 3
auto_find_batch_size: True
full_determinism: True
lr_scheduler_type: linear
label_names: ["Not offensive", "Offensive"]
fp16: False
